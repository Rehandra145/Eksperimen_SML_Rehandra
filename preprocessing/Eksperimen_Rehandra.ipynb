{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ],
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:\n",
        "\n",
        "1. **Sumber Dataset**:  \n",
        "   Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.\n",
        "\n",
        "**Dataset yang digunakan:** Heart Disease Prediction Dataset\n",
        "- Sumber: UCI ML Repository / Kaggle\n",
        "- Jumlah data: 270 baris\n",
        "- Jumlah fitur: 14 kolom (13 fitur + 1 target)\n",
        "- Target: Heart Disease (Presence/Absence)"
      ],
      "metadata": {
        "id": "hssSDn-5n3HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import Library**"
      ],
      "metadata": {
        "id": "fKADPWcFKlj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning."
      ],
      "metadata": {
        "id": "LgA3ERnVn84N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library yang diperlukan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Konfigurasi\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Library berhasil diimport!\")"
      ],
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Memuat Dataset**"
      ],
      "metadata": {
        "id": "f3YIEnAFKrKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
        "\n",
        "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.\n",
        "\n",
        "Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan"
      ],
      "metadata": {
        "id": "Ey3ItwTen_7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset\n",
        "df = pd.read_csv('../dataset_raw/Heart_Disease_Prediction.csv')\n",
        "\n",
        "# Menampilkan informasi dasar dataset\n",
        "print(\"=\"*60)\n",
        "print(\"INFORMASI DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nShape dataset: {df.shape}\")\n",
        "print(f\"Jumlah baris: {df.shape[0]}\")\n",
        "print(f\"Jumlah kolom: {df.shape[1]}\")\n",
        "\n",
        "# Menampilkan 5 baris pertama\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"5 BARIS PERTAMA DATASET\")\n",
        "print(\"=\"*60)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan 5 baris terakhir dan info tipe data\n",
        "print(\"5 BARIS TERAKHIR DATASET\")\n",
        "display(df.tail())\n",
        "\n",
        "print(\"\\nINFORMASI TIPE DATA\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "dataset_info"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.\n",
        "\n",
        "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
      ],
      "metadata": {
        "id": "bgZkbJLpK9UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Statistik Deskriptif\n",
        "print(\"=\"*60)\n",
        "print(\"4.1 STATISTIK DESKRIPTIF\")\n",
        "print(\"=\"*60)\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.2 Analisis Missing Values & Duplikat\n",
        "print(\"=\"*60)\n",
        "print(\"4.2 ANALISIS MISSING VALUES\")\n",
        "print(\"=\"*60)\n",
        "missing_values = df.isnull().sum()\n",
        "print(f\"Total missing values: {missing_values.sum()}\")\n",
        "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"Tidak ada missing values\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"4.3 ANALISIS DATA DUPLIKAT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Jumlah baris duplikat: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "eda_missing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4 Distribusi Target Variable\n",
        "print(\"=\"*60)\n",
        "print(\"4.4 DISTRIBUSI TARGET VARIABLE\")\n",
        "print(\"=\"*60)\n",
        "print(df['Heart Disease'].value_counts())\n",
        "print(f\"\\nPersentase:\")\n",
        "print(df['Heart Disease'].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualisasi\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "colors = ['#66b3ff', '#ff6666']\n",
        "df['Heart Disease'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[0], colors=colors)\n",
        "axes[0].set_title('Distribusi Heart Disease')\n",
        "df['Heart Disease'].value_counts().plot(kind='bar', ax=axes[1], color=colors, edgecolor='black')\n",
        "axes[1].set_title('Frekuensi Heart Disease')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eda_target"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.5 Histogram Fitur Numerik\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "n_cols, n_rows = 4, (len(numeric_cols) + 3) // 4\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    df[col].hist(bins=20, ax=axes[i], color='steelblue', edgecolor='black')\n",
        "    axes[i].set_title(f'Distribusi {col}')\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eda_hist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.6 Correlation Matrix\n",
        "df_encoded = df.copy()\n",
        "le = LabelEncoder()\n",
        "df_encoded['Heart Disease'] = le.fit_transform(df['Heart Disease'])\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(df_encoded.corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eda_corr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.7 Boxplot Deteksi Outlier\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    sns.boxplot(data=df, y=col, ax=axes[i], color='steelblue')\n",
        "    axes[i].set_title(f'Boxplot {col}')\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eda_boxplot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Data Preprocessing**"
      ],
      "metadata": {
        "id": "cpgHfgnSK3ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning.\n",
        "\n",
        "Jika Anda menggunakan data teks, data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.\n",
        "\n",
        "Berikut adalah tahapan-tahapan yang bisa dilakukan, tetapi **tidak terbatas** pada:\n",
        "1. Menghapus atau Menangani Data Kosong (Missing Values)\n",
        "2. Menghapus Data Duplikat\n",
        "3. Normalisasi atau Standarisasi Fitur\n",
        "4. Deteksi dan Penanganan Outlier\n",
        "5. Encoding Data Kategorikal\n",
        "6. Binning (Pengelompokan Data)\n",
        "\n",
        "Cukup sesuaikan dengan karakteristik data yang kamu gunakan yah. Khususnya ketika kami menggunakan data tidak terstruktur."
      ],
      "metadata": {
        "id": "COf8KUPXLg5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 Menangani Missing Values\n",
        "print(\"=\"*60)\n",
        "print(\"5.1 MENANGANI MISSING VALUES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if df.isnull().sum().sum() > 0:\n",
        "    for col in numeric_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "    print(\"Missing values ditangani dengan median.\")\n",
        "else:\n",
        "    print(\"Tidak ada missing values.\")\n",
        "\n",
        "# 5.2 Menghapus Duplikat\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"5.2 MENGHAPUS DUPLIKAT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dup_count = df.duplicated().sum()\n",
        "if dup_count > 0:\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    print(f\"Menghapus {dup_count} baris duplikat.\")\n",
        "else:\n",
        "    print(\"Tidak ada duplikat.\")"
      ],
      "metadata": {
        "id": "prep_missing_dup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.3 Penanganan Outlier (IQR Capping)\n",
        "print(\"=\"*60)\n",
        "print(\"5.3 PENANGANAN OUTLIER (IQR Capping)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def handle_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "    outliers = ((data[column] < lower) | (data[column] > upper)).sum()\n",
        "    if outliers > 0:\n",
        "        data[column] = np.clip(data[column], lower, upper)\n",
        "        print(f\"  - {column}: {outliers} outliers di-capping\")\n",
        "    return data\n",
        "\n",
        "for col in numeric_cols:\n",
        "    df = handle_outliers_iqr(df, col)"
      ],
      "metadata": {
        "id": "prep_outlier"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.4 Encoding Kategorikal\n",
        "print(\"=\"*60)\n",
        "print(\"5.4 ENCODING KATEGORIKAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "le_target = LabelEncoder()\n",
        "df['Heart Disease'] = le_target.fit_transform(df['Heart Disease'])\n",
        "print(f\"Heart Disease encoded: {dict(zip(le_target.classes_, [0, 1]))}\")"
      ],
      "metadata": {
        "id": "prep_encoding"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.5 Normalisasi (StandardScaler)\n",
        "print(\"=\"*60)\n",
        "print(\"5.5 NORMALISASI (StandardScaler)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X = df.drop('Heart Disease', axis=1)\n",
        "y = df['Heart Disease']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "print(f\"Shape X: {X_scaled.shape}\")\n",
        "print(f\"Shape y: {y.shape}\")\n",
        "print(\"\\nStatistik setelah scaling (mean ~ 0, std ~ 1):\")\n",
        "print(X_scaled.describe().loc[['mean', 'std']])"
      ],
      "metadata": {
        "id": "prep_scaling"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.6 Train-Test Split\n",
        "print(\"=\"*60)\n",
        "print(\"5.6 TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")\n",
        "print(f\"\\nDistribusi target - Train: {dict(pd.Series(y_train).value_counts())}\")\n",
        "print(f\"Distribusi target - Test: {dict(pd.Series(y_test).value_counts())}\")"
      ],
      "metadata": {
        "id": "prep_split"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.7 Simpan Data Siap Latih\n",
        "print(\"=\"*60)\n",
        "print(\"5.7 SIMPAN DATA SIAP LATIH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "output_dir = 'dataset_preprocessing'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Gabungkan X dan y untuk disimpan\n",
        "train_data = X_train.copy()\n",
        "train_data['Heart Disease'] = y_train.values\n",
        "\n",
        "test_data = X_test.copy()\n",
        "test_data['Heart Disease'] = y_test.values\n",
        "\n",
        "# Simpan ke CSV\n",
        "train_data.to_csv(f'{output_dir}/train_data.csv', index=False)\n",
        "test_data.to_csv(f'{output_dir}/test_data.csv', index=False)\n",
        "\n",
        "print(f\"✅ train_data.csv disimpan ({train_data.shape[0]} baris)\")\n",
        "print(f\"✅ test_data.csv disimpan ({test_data.shape[0]} baris)\")\n",
        "print(f\"\\nLokasi: {output_dir}/\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING SELESAI!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}